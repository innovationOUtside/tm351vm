2018-04-30T18:35:23.853+0000 I CONTROL  [main] 
2018-04-30T18:35:23.853+0000 I CONTROL  [main] ** WARNING: Access control is not enabled for the database.
2018-04-30T18:35:23.853+0000 I CONTROL  [main] **          Read and write access to data and configuration is unrestricted.
2018-04-30T18:35:23.853+0000 I CONTROL  [main] ** WARNING: You are running this process as the root user, which is not recommended.
2018-04-30T18:35:23.854+0000 I CONTROL  [main] 
2018-04-30T18:35:23.854+0000 I CONTROL  [main] ** WARNING: This server is bound to localhost.
2018-04-30T18:35:23.854+0000 I CONTROL  [main] **          Remote systems will be unable to connect to this server. 
2018-04-30T18:35:23.854+0000 I CONTROL  [main] **          Start the server with --bind_ip <address> to specify which IP 
2018-04-30T18:35:23.854+0000 I CONTROL  [main] **          addresses it should serve responses from, or with --bind_ip_all to
2018-04-30T18:35:23.854+0000 I CONTROL  [main] **          bind to all interfaces. If this behavior is desired, start the
2018-04-30T18:35:23.854+0000 I CONTROL  [main] **          server with --bind_ip 127.0.0.1 to disable this warning.
2018-04-30T18:35:23.855+0000 I CONTROL  [main] 
2018-04-30T18:35:23.857+0000 I SHARDING [mongosMain] mongos version v3.6.4
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain] git version: d0181a711f7e7f39e60b5aeb1dc7097bf6ae5856
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain] OpenSSL version: OpenSSL 1.0.2g  1 Mar 2016
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain] allocator: tcmalloc
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain] modules: none
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain] build environment:
2018-04-30T18:35:23.857+0000 I CONTROL  [mongosMain]     distmod: ubuntu1604
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain]     distarch: x86_64
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain]     target_arch: x86_64
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] db version v3.6.4
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] git version: d0181a711f7e7f39e60b5aeb1dc7097bf6ae5856
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] OpenSSL version: OpenSSL 1.0.2g  1 Mar 2016
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] allocator: tcmalloc
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] modules: none
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] build environment:
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain]     distmod: ubuntu1604
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain]     distarch: x86_64
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain]     target_arch: x86_64
2018-04-30T18:35:23.858+0000 I CONTROL  [mongosMain] options: { processManagement: { fork: true, pidFilePath: "/vagrant/logs/mongocluster_mongos-2.pid" }, sharding: { configDB: "c1/localhost:57050" }, systemLog: { destination: "file", path: "/vagrant/logs/mongocluster_mongos-2.log" } }
2018-04-30T18:35:23.860+0000 I NETWORK  [mongosMain] Starting new replica set monitor for c1/localhost:57050
2018-04-30T18:35:23.862+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to localhost:57050 (1 connections now open to localhost:57050 with a 5 second timeout)
2018-04-30T18:35:23.862+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57050
2018-04-30T18:35:23.862+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57050
2018-04-30T18:35:23.862+0000 I SHARDING [thread1] creating distributed lock ping thread for process tm351-18J-test-student:27017:1525113323:-3961489942162703859 (sleeping for 30000ms)
2018-04-30T18:35:23.864+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57050, took 2ms (2 connections now open to localhost:57050)
2018-04-30T18:35:23.864+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57050
2018-04-30T18:35:23.866+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57050, took 4ms (3 connections now open to localhost:57050)
2018-04-30T18:35:23.867+0000 W SHARDING [replSetDistLockPinger] pinging failed for distributed lock pinger :: caused by :: LockStateChangeFailed: findAndModify query predicate didn't match any lock document
2018-04-30T18:35:23.867+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57050, took 3ms (3 connections now open to localhost:57050)
2018-04-30T18:35:25.867+0000 I FTDC     [mongosMain] Initializing full-time diagnostic data capture with directory '/vagrant/logs/mongocluster_mongos-2.diagnostic.data'
2018-04-30T18:35:25.868+0000 I NETWORK  [mongosMain] waiting for connections on port 27017
2018-04-30T18:35:31.266+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33766 #1 (1 connection now open)
2018-04-30T18:35:31.286+0000 I NETWORK  [conn1] received client metadata from 127.0.0.1:33766 conn1: { driver: { name: "PyMongo", version: "3.6.1" }, os: { type: "Linux", name: "Linux", architecture: "x86_64", version: "4.4.0-87-generic" }, platform: "CPython 3.5.2.final.0" }
2018-04-30T18:35:31.376+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33768 #2 (2 connections now open)
2018-04-30T18:35:31.389+0000 I NETWORK  [conn2] received client metadata from 127.0.0.1:33768 conn2: { driver: { name: "PyMongo", version: "3.6.1" }, os: { type: "Linux", name: "Linux", architecture: "x86_64", version: "4.4.0-87-generic" }, platform: "CPython 3.5.2.final.0" }
2018-04-30T18:35:58.111+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33776 #3 (3 connections now open)
2018-04-30T18:35:58.111+0000 I NETWORK  [conn3] received client metadata from 127.0.0.1:33776 conn3: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "3.6.4" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2018-04-30T18:35:58.143+0000 I NETWORK  [conn3] Starting new replica set monitor for s0/localhost:37017
2018-04-30T18:35:58.147+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to localhost:37017 (1 connections now open to localhost:37017 with a 5 second timeout)
2018-04-30T18:35:58.164+0000 I NETWORK  [conn3] Starting new replica set monitor for s1/localhost:47017
2018-04-30T18:35:58.167+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to localhost:47017 (1 connections now open to localhost:47017 with a 5 second timeout)
2018-04-30T18:35:58.182+0000 I NETWORK  [conn3] Starting new replica set monitor for s2/localhost:57017
2018-04-30T18:35:58.188+0000 I SHARDING [conn3] distributed lock 'accidents-movePrimary' acquired for 'dropDatabase', ts : 5ae7620ea5a8b7877c7fefe1
2018-04-30T18:35:58.189+0000 I SHARDING [conn3] distributed lock 'accidents' acquired for 'dropDatabase', ts : 5ae7620ea5a8b7877c7fefe2
2018-04-30T18:35:58.190+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to localhost:57017 (1 connections now open to localhost:57017 with a 5 second timeout)
2018-04-30T18:35:58.198+0000 I SHARDING [conn3] distributed lock with ts: 5ae7620ea5a8b7877c7fefe2' unlocked.
2018-04-30T18:35:58.199+0000 I SHARDING [conn3] distributed lock with ts: 5ae7620ea5a8b7877c7fefe1' unlocked.
2018-04-30T18:35:58.401+0000 I NETWORK  [conn3] end connection 127.0.0.1:33776 (2 connections now open)
2018-04-30T18:36:23.869+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Ending idle connection to host localhost:57050 because the pool meets constraints; 2 connections to that host remain open
2018-04-30T18:36:23.870+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Ending idle connection to host localhost:57050 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:36:23.870+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57050
2018-04-30T18:36:23.875+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57050, took 5ms (2 connections now open to localhost:57050)
2018-04-30T18:36:42.742+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33882 #4 (3 connections now open)
2018-04-30T18:36:42.766+0000 I NETWORK  [conn4] Successfully connected to s0/localhost:37017 (1 connections now open to s0/localhost:37017 with a 0 second timeout)
2018-04-30T18:36:42.777+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Connecting to localhost:37017
2018-04-30T18:36:42.778+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Successfully connected to localhost:37017, took 1ms (1 connections now open to localhost:37017)
2018-04-30T18:36:42.784+0000 I NETWORK  [conn4] Successfully connected to s0/localhost:37017 (1 connections now open to s0/localhost:37017 with a 0 second timeout)
2018-04-30T18:36:42.795+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33894 #5 (4 connections now open)
2018-04-30T18:36:42.810+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33898 #6 (5 connections now open)
2018-04-30T18:36:42.812+0000 I SHARDING [conn5] Refreshing chunks for collection accidents.roads based on version 0|0||000000000000000000000000
2018-04-30T18:36:42.819+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33902 #7 (6 connections now open)
2018-04-30T18:36:42.822+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.roads took 12 ms and found version 1|0||5ae7620e1b1b0bfabb4ae712
2018-04-30T18:36:42.831+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Connecting to localhost:37017
2018-04-30T18:36:42.832+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33906 #8 (7 connections now open)
2018-04-30T18:36:42.832+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Connecting to localhost:37017
2018-04-30T18:36:42.844+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Connecting to localhost:37017
2018-04-30T18:36:42.849+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Connecting to localhost:37017
2018-04-30T18:36:42.849+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Successfully connected to localhost:37017, took 17ms (1 connections now open to localhost:37017)
2018-04-30T18:36:42.851+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Successfully connected to localhost:37017, took 20ms (2 connections now open to localhost:37017)
2018-04-30T18:36:42.858+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Successfully connected to localhost:37017, took 14ms (2 connections now open to localhost:37017)
2018-04-30T18:36:42.860+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Successfully connected to localhost:37017, took 11ms (1 connections now open to localhost:37017)
2018-04-30T18:36:42.871+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Connecting to localhost:37017
2018-04-30T18:36:42.890+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Successfully connected to localhost:37017, took 19ms (2 connections now open to localhost:37017)
2018-04-30T18:36:42.924+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 0|0||000000000000000000000000
2018-04-30T18:36:42.938+0000 I NETWORK  [listener] connection accepted from 127.0.0.1:33930 #9 (8 connections now open)
2018-04-30T18:36:42.953+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 28 ms and found version 1|0||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:42.988+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Connecting to localhost:37017
2018-04-30T18:36:43.022+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Successfully connected to localhost:37017, took 34ms (2 connections now open to localhost:37017)
2018-04-30T18:36:43.080+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Connecting to localhost:37017
2018-04-30T18:36:43.107+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Connecting to localhost:37017
2018-04-30T18:36:43.132+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Successfully connected to localhost:37017, took 52ms (3 connections now open to localhost:37017)
2018-04-30T18:36:43.142+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Successfully connected to localhost:37017, took 35ms (2 connections now open to localhost:37017)
2018-04-30T18:36:43.736+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:37017
2018-04-30T18:36:43.752+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:37017, took 16ms (1 connections now open to localhost:37017)
2018-04-30T18:36:44.859+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Connecting to localhost:37017
2018-04-30T18:36:44.881+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Successfully connected to localhost:37017, took 22ms (3 connections now open to localhost:37017)
2018-04-30T18:36:46.824+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:37017
2018-04-30T18:36:46.838+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:37017
2018-04-30T18:36:46.849+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:37017, took 25ms (3 connections now open to localhost:37017)
2018-04-30T18:36:46.853+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:37017, took 15ms (3 connections now open to localhost:37017)
2018-04-30T18:36:46.910+0000 I SHARDING [conn8] autosplitted accidents.accidents chunk: shard: s0, lastmod: 1|0||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: MinKey }, { Accident_Index: MaxKey }) into 3 parts (desiredChunkSize 67108864)
2018-04-30T18:36:46.910+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 1|0||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:46.911+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 1|3||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:47.056+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(1, 0), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: MinKey }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200901BS70002" }, { Accident_Index: "200920H021402" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: MinKey }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:47.072+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(1, 0), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: MinKey }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200901BS70002" }, { Accident_Index: "200920H021402" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: MinKey }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:47.521+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 1|3||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:47.543+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 22 ms and found version 2|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:48.183+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Connecting to localhost:47017
2018-04-30T18:36:48.183+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Connecting to localhost:57017
2018-04-30T18:36:48.187+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Successfully connected to localhost:57017, took 4ms (1 connections now open to localhost:57017)
2018-04-30T18:36:48.187+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Successfully connected to localhost:47017, took 4ms (1 connections now open to localhost:47017)
2018-04-30T18:36:49.506+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s0, lastmod: 2|1||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "200901BS70002" }, { Accident_Index: "200920H031501" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:36:49.507+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 2|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:49.508+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 2|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:49.573+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 1), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200901BS70002" }, max: { Accident_Index: "200920H031501" }, splitKeys: [ { Accident_Index: "200901RY10434" }, { Accident_Index: "200905AA31739" } ] } failed :: caused by :: BadValue: chunk operation commit failed: version 2|7||5ae7620e1b1b0bfabb4ae6f5 doesn't exist in namespace: accidents.accidents. Unable to save chunk ops. Command: { applyOps: [ { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"200901BS70002"", lastmod: Timestamp(2, 5), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "200901BS70002" }, max: { Accident_Index: "200901RY10434" }, shard: "s0" }, o2: { _id: "accidents.accidents-Accident_Index_"200901BS70002"" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"200901RY10434"", lastmod: Timestamp(2, 6), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "200901RY10434" }, max: { Accident_Index: "200905AA31739" }, shard: "s0" }, o2: { _id: "accidents.accidents-Accident_Index_"200901RY10434"" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"200905AA31739"", lastmod: Timestamp(2, 7), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "200905AA31739" }, max: { Accident_Index: "200920H031501" }, shard: "s0" }, o2: { _id: "accidents.accidents-Accident_Index_"200905AA31739"" } } ], preCondition: [ { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: "200901BS70002" }, max: { Accident_Index: "200920H031501" } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s0" } } ], writeConcern: { w: 0, wtimeout: 0 } }. Result: { got: {}, whatFailed: { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: "200901BS70002" }, max: { Accident_Index: "200920H031501" } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s0" } }, ok: 0.0, errmsg: "preCondition failed", code: 2, codeName: "BadValue", operationTime: Timestamp(1525113409, 6201), $gleStats: { lastOpTime: { ts: Timestamp(1525113409, 6201), t: 1 }, electionId: ObjectId('7fffffff0000000000000001') }, $clusterTime: { clusterTime: Timestamp(1525113409, 6264), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } } :: caused by :: preCondition failed
2018-04-30T18:36:51.479+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D042709" }, { Accident_Index: "201001YE80578" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:52.068+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D038609" }, { Accident_Index: "201004EB10017" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:52.104+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D038609" }, { Accident_Index: "201004EB10017" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:52.117+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D038609" }, { Accident_Index: "201004EB10017" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:52.768+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Connecting to localhost:57017
2018-04-30T18:36:52.778+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Successfully connected to localhost:57017, took 9ms (1 connections now open to localhost:57017)
2018-04-30T18:36:52.936+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 2|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:52.937+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 2|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:36:53.523+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D027909" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:54.044+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D016009" }, { Accident_Index: "201001RG40728" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:54.146+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D015309" }, { Accident_Index: "201001QK50605" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:54.843+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D007909" }, { Accident_Index: "201001QK50558" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:54.903+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D007009" }, { Accident_Index: "201001QK50544" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:55.011+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932D007009" }, { Accident_Index: "201001QK50544" }, { Accident_Index: "2010170S12770" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:55.626+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C395709" }, { Accident_Index: "201001QK50497" }, { Accident_Index: "201004EB10091" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:56.273+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C392509" }, { Accident_Index: "201001QK50487" }, { Accident_Index: "201004EB10076" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:56.316+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C392509" }, { Accident_Index: "201001QK50487" }, { Accident_Index: "201004EB10076" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:56.436+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Connecting to localhost:57017
2018-04-30T18:36:56.492+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C392509" }, { Accident_Index: "201001QK50487" }, { Accident_Index: "201004EB10076" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:56.508+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Successfully connected to localhost:57017, took 70ms (1 connections now open to localhost:57017)
2018-04-30T18:36:56.641+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Connecting to localhost:57017
2018-04-30T18:36:56.657+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Successfully connected to localhost:57017, took 16ms (1 connections now open to localhost:57017)
2018-04-30T18:36:57.335+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C383109" }, { Accident_Index: "200943N180039" }, { Accident_Index: "201001XD80782" }, { Accident_Index: "201006K029295" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:57.348+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C380109" }, { Accident_Index: "200943N176129" }, { Accident_Index: "201001XD80763" }, { Accident_Index: "201006K028641" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:57.352+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C383109" }, { Accident_Index: "200943N180039" }, { Accident_Index: "201001XD80782" }, { Accident_Index: "201006K029295" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:58.024+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57017
2018-04-30T18:36:58.034+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57017, took 10ms (1 connections now open to localhost:57017)
2018-04-30T18:36:58.886+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C374209" }, { Accident_Index: "200943N173089" }, { Accident_Index: "201001RG40814" }, { Accident_Index: "201004FL10059" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:58.886+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C374209" }, { Accident_Index: "200943N173089" }, { Accident_Index: "201001RG40814" }, { Accident_Index: "201004FL10059" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:36:58.943+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C374209" }, { Accident_Index: "200943N173089" }, { Accident_Index: "201001RG40814" }, { Accident_Index: "201004FL10059" }, { Accident_Index: "201042I107504" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:00.234+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C372409" }, { Accident_Index: "200943N171079" }, { Accident_Index: "201001QK50665" }, { Accident_Index: "201004EL10092" }, { Accident_Index: "201014K192510" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:00.311+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C372409" }, { Accident_Index: "200943N171079" }, { Accident_Index: "201001QK50665" }, { Accident_Index: "201004EL10092" }, { Accident_Index: "201014K192510" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:00.351+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C372409" }, { Accident_Index: "200943N171079" }, { Accident_Index: "201001QK50665" }, { Accident_Index: "201004EL10092" }, { Accident_Index: "201014K192510" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:01.658+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C369209" }, { Accident_Index: "200943N170049" }, { Accident_Index: "201001MM70091" }, { Accident_Index: "201004AL10003" }, { Accident_Index: "201011SB30470" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:01.732+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C369209" }, { Accident_Index: "200943N170049" }, { Accident_Index: "201001LX51151" }, { Accident_Index: "201001ZT80316" }, { Accident_Index: "201011SA18690" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:01.795+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C369209" }, { Accident_Index: "200943N170049" }, { Accident_Index: "201001LX51151" }, { Accident_Index: "201001ZT80316" }, { Accident_Index: "201011SA18690" }, { Accident_Index: "201043P121070" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:02.867+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C362109" }, { Accident_Index: "200943N167109" }, { Accident_Index: "200954DE17009" }, { Accident_Index: "201001XD80883" }, { Accident_Index: "201006K030743" }, { Accident_Index: "201020K727711" }, { Accident_Index: "201043S126120" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:02.878+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C362109" }, { Accident_Index: "200943N167109" }, { Accident_Index: "200954DE17009" }, { Accident_Index: "201001XD80883" }, { Accident_Index: "201006K030743" }, { Accident_Index: "201020K727711" }, { Accident_Index: "201043S126120" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:03.021+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(2, 4), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C358409" }, { Accident_Index: "200943N164089" }, { Accident_Index: "200954DE12009" }, { Accident_Index: "201001XD80856" }, { Accident_Index: "201006K030109" }, { Accident_Index: "201020K041912" }, { Accident_Index: "201043W156080" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:04.163+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 2|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:04.174+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 10 ms and found version 3|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:04.386+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(3, 1), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C369209" }, { Accident_Index: "200943N170049" }, { Accident_Index: "200954DE23209" }, { Accident_Index: "201001RG40372" }, { Accident_Index: "201004FB10006" }, { Accident_Index: "201014K298910" }, { Accident_Index: "201043W288100" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:04.387+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s0, lastmod: 1|3||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey }) into 8 parts (desiredChunkSize 67108864)
2018-04-30T18:37:04.388+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 3|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:04.390+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 2 ms and found version 3|9||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:04.525+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(3, 1), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "200920H031501" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "200932C369209" }, { Accident_Index: "200943N170049" }, { Accident_Index: "200954DE23209" }, { Accident_Index: "201001RG40372" }, { Accident_Index: "201004FB10006" }, { Accident_Index: "201014K298910" }, { Accident_Index: "201043W288100" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "200920H031501" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:04.985+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 3|9||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:04.988+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 3 ms and found version 4|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:05.039+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 4|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:05.040+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 4|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:07.416+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s0", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(4, 1), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "201014K298910" }, max: { Accident_Index: "201043W288100" }, splitKeys: [ { Accident_Index: "201030C000779" }, { Accident_Index: "201043N265010" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "201014K298910" }, { Accident_Index: "201043W288100" })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:08.822+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57050
2018-04-30T18:37:08.948+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57050, took 129ms (3 connections now open to localhost:57050)
2018-04-30T18:37:10.287+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57017
2018-04-30T18:37:10.308+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Connecting to localhost:57017
2018-04-30T18:37:10.345+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57017, took 35ms (3 connections now open to localhost:57017)
2018-04-30T18:37:10.345+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Successfully connected to localhost:57017, took 65ms (3 connections now open to localhost:57017)
2018-04-30T18:37:31.552+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Detected bad connection created at 1525113358144803 microSec, clearing pool for localhost:37017 of 0 connections
2018-04-30T18:37:31.553+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Dropping all pooled connections to localhost:37017(with timeout of 5 seconds)
2018-04-30T18:37:31.553+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Ending connection to host localhost:37017(with timeout of 5 seconds) due to bad connection status; 0 connections to that host remain open
2018-04-30T18:37:31.556+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host localhost:37017 as failed :: caused by :: HostUnreachable: network error while attempting to run command 'ismaster' on host 'localhost:37017' 
2018-04-30T18:37:31.557+0000 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Unable to reach primary for set s0
2018-04-30T18:37:31.557+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 1 checks in a row.
2018-04-30T18:37:31.698+0000 I NETWORK  [conn8] Successfully connected to s2/localhost:57017 (1 connections now open to s2/localhost:57017 with a 0 second timeout)
2018-04-30T18:37:31.745+0000 I SHARDING [conn8] autosplitted accidents.accidents chunk: shard: s2, lastmod: 2|0||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: MinKey }, { Accident_Index: "200901BS70002" }) into 3 parts (desiredChunkSize 67108864)
2018-04-30T18:37:31.745+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 4|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:31.751+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 5 ms and found version 4|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:31.824+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s2", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(4, 1), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: MinKey }, max: { Accident_Index: "200901BS70002" }, splitKeys: [ { Accident_Index: 2009030000008 }, { Accident_Index: 2010471000846 } ] } failed :: caused by :: BadValue: chunk operation commit failed: version 4|7||5ae7620e1b1b0bfabb4ae6f5 doesn't exist in namespace: accidents.accidents. Unable to save chunk ops. Command: { applyOps: [ { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_MinKey", lastmod: Timestamp(4, 5), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: MinKey }, max: { Accident_Index: 2009030000008 }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_MinKey" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_2009030000008", lastmod: Timestamp(4, 6), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: 2009030000008 }, max: { Accident_Index: 2010471000846 }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_2009030000008" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_2010471000846", lastmod: Timestamp(4, 7), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: 2010471000846 }, max: { Accident_Index: "200901BS70002" }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_2010471000846" } } ], preCondition: [ { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: MinKey }, max: { Accident_Index: "200901BS70002" } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s2" } } ], writeConcern: { w: 0, wtimeout: 0 } }. Result: { got: {}, whatFailed: { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: MinKey }, max: { Accident_Index: "200901BS70002" } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s2" } }, ok: 0.0, errmsg: "preCondition failed", code: 2, codeName: "BadValue", operationTime: Timestamp(1525113451, 1599), $gleStats: { lastOpTime: { ts: Timestamp(1525113451, 1599), t: 1 }, electionId: ObjectId('7fffffff0000000000000001') }, $clusterTime: { clusterTime: Timestamp(1525113451, 1599), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } } :: caused by :: preCondition failed
2018-04-30T18:37:34.185+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|3||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2009030000008 }, { Accident_Index: 2010471001362 }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:34.185+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:34.187+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|7||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:36.317+0000 I SHARDING [conn9] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|0||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201043W288100" }, { Accident_Index: MaxKey }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:36.317+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 4|7||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:36.318+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 4|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:38.289+0000 I SHARDING [conn9] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|9||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201063DP02810" }, { Accident_Index: "201106A046880" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:38.289+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 4|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:38.291+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|13||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:38.716+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|4||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2010471001362 }, { Accident_Index: "200901BS70002" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:38.716+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|13||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:38.717+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 4|16||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:43.699+0000 I SHARDING [conn9] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|10||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201106A046880" }, { Accident_Index: MaxKey }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:43.700+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 4|16||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:43.701+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|19||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:43.994+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 2 connections to that host remain open
2018-04-30T18:37:44.663+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|16||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2011130097914 }, { Accident_Index: "200901BS70002" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:44.663+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|19||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:44.664+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|22||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:44.883+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 2 connections to that host remain open
2018-04-30T18:37:45.553+0000 I SHARDING [conn9] Split chunk { splitChunk: "accidents.accidents", from: "s2", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(4, 22), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "201120H007761" }, max: { Accident_Index: "201201EO40786" }, splitKeys: [ { Accident_Index: "201132C061011" }, { Accident_Index: "201143P294031" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "201120H007761" }, { Accident_Index: "201201EO40786" })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:37:45.554+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|18||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201120H007761" }, { Accident_Index: "201201EO40786" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:45.554+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|22||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:45.556+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|25||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:47.799+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:37:48.132+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:37:48.173+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:37:48.180+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:37:48.715+0000 I SHARDING [conn9] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|19||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201201EO40786" }, { Accident_Index: MaxKey }) into 3 parts (desiredChunkSize 67108864)
2018-04-30T18:37:48.715+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 4|25||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:48.717+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 4|28||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:48.743+0000 I SHARDING [conn4] Split chunk { splitChunk: "accidents.accidents", from: "s2", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(4, 25), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "201201EO40786" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "201201TX20399" }, { Accident_Index: "201206F059845" } ] } failed :: caused by :: BadValue: chunk operation commit failed: version 4|31||5ae7620e1b1b0bfabb4ae6f5 doesn't exist in namespace: accidents.accidents. Unable to save chunk ops. Command: { applyOps: [ { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"201201EO40786"", lastmod: Timestamp(4, 29), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "201201EO40786" }, max: { Accident_Index: "201201TX20399" }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_"201201EO40786"" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"201201TX20399"", lastmod: Timestamp(4, 30), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "201201TX20399" }, max: { Accident_Index: "201206F059845" }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_"201201TX20399"" } }, { op: "u", b: true, ns: "config.chunks", o: { _id: "accidents.accidents-Accident_Index_"201206F059845"", lastmod: Timestamp(4, 31), lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), ns: "accidents.accidents", min: { Accident_Index: "201206F059845" }, max: { Accident_Index: MaxKey }, shard: "s2" }, o2: { _id: "accidents.accidents-Accident_Index_"201206F059845"" } } ], preCondition: [ { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: "201201EO40786" }, max: { Accident_Index: MaxKey } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s2" } } ], writeConcern: { w: 0, wtimeout: 0 } }. Result: { got: {}, whatFailed: { ns: "config.chunks", q: { query: { ns: "accidents.accidents", min: { Accident_Index: "201201EO40786" }, max: { Accident_Index: MaxKey } }, orderby: { lastmod: -1 } }, res: { lastmodEpoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shard: "s2" } }, ok: 0.0, errmsg: "preCondition failed", code: 2, codeName: "BadValue", operationTime: Timestamp(1525113468, 10914), $gleStats: { lastOpTime: { ts: Timestamp(1525113468, 10914), t: 1 }, electionId: ObjectId('7fffffff0000000000000001') }, $clusterTime: { clusterTime: Timestamp(1525113468, 10914), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } } :: caused by :: preCondition failed
2018-04-30T18:37:50.443+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|7||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2009360051782 }, { Accident_Index: 2010471001362 }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:50.443+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|28||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:50.444+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 4|31||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:51.811+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|22||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2011471101698 }, { Accident_Index: "200901BS70002" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:37:51.812+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|31||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:51.812+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 4|34||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:59.788+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 4|34||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:59.790+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 5|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:59.806+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 5|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:37:59.809+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 3 ms and found version 5|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:00.195+0000 I SHARDING [conn8] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|34||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2012130150220 }, { Accident_Index: "200901BS70002" }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:38:00.195+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 5|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:00.197+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 5|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:00.551+0000 I SHARDING [conn8] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|31||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: 2009930001203 }, { Accident_Index: 2010471001362 }) into 3 parts (desiredChunkSize 67108864) (migrate suggested)
2018-04-30T18:38:00.552+0000 I SHARDING [conn8] Refreshing chunks for collection accidents.accidents based on version 5|4||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:00.553+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 5|7||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:01.598+0000 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:01.598+0000 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Unable to reach primary for set s0
2018-04-30T18:38:01.598+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 2 checks in a row.
2018-04-30T18:38:01.611+0000 I SHARDING [conn8] Split chunk { splitChunk: "accidents.accidents", from: "s2", keyPattern: { Accident_Index: 1.0 }, epoch: ObjectId('5ae7620e1b1b0bfabb4ae6f5'), shardVersion: [ Timestamp(5, 7), ObjectId('5ae7620e1b1b0bfabb4ae6f5') ], min: { Accident_Index: "201206F059845" }, max: { Accident_Index: MaxKey }, splitKeys: [ { Accident_Index: "201220S005562" }, { Accident_Index: "201242I193506" } ] } failed :: caused by :: LockBusy: could not acquire collection lock for accidents.accidents to split chunk [{ Accident_Index: "201206F059845" }, { Accident_Index: MaxKey })  :: caused by :: LockBusy: timed out waiting for accidents.accidents
2018-04-30T18:38:01.622+0000 I SHARDING [conn4] autosplitted accidents.accidents chunk: shard: s2, lastmod: 4|28||5ae7620e1b1b0bfabb4ae6f5, [{ Accident_Index: "201206F059845" }, { Accident_Index: MaxKey }) into 3 parts (desiredChunkSize 67108864)
2018-04-30T18:38:01.622+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 5|7||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:01.623+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 1 ms and found version 5|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:04.386+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 2 connections to that host remain open
2018-04-30T18:38:08.820+0000 I SHARDING [conn9] Unable to auto-split chunk [{ Accident_Index: "201201EO40786" }, { Accident_Index: MaxKey }) :: caused by :: StaleShardVersion: Unable to move chunk with arguments 'ns: accidents.accidents, [{ Accident_Index: "201206F059845" }, { Accident_Index: MaxKey }), fromShard: s2, toShard: s1' due to error Unable to find chunk with the exact bounds [{ Accident_Index: "201206F059845" }, { Accident_Index: MaxKey }) at collection version 5|10||5ae7620e1b1b0bfabb4ae6f5, going to invalidate routing table entry for accidents.accidents
2018-04-30T18:38:08.820+0000 I SHARDING [conn9] Refreshing chunks for collection accidents.accidents based on version 5|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:08.821+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 0 ms and found version 5|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:08.959+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2018-04-30T18:38:08.961+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Failed asio heartbeat to localhost:37017 - HostUnreachable: End of file
2018-04-30T18:38:08.961+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Dropping all pooled connections to localhost:37017 due to failed operation on a connection
2018-04-30T18:38:08.961+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Failed to close stream: Transport endpoint is not connected
2018-04-30T18:38:10.346+0000 I ASIO     [NetworkInterfaceASIO-ShardRegistry-0] Ending idle connection to host localhost:57017 because the pool meets constraints; 2 connections to that host remain open
2018-04-30T18:38:13.446+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Failed asio heartbeat to localhost:37017 - HostUnreachable: End of file
2018-04-30T18:38:13.446+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Dropping all pooled connections to localhost:37017 due to failed operation on a connection
2018-04-30T18:38:13.446+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-2-0] Failed to close stream: Transport endpoint is not connected
2018-04-30T18:38:15.828+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Failed asio heartbeat to localhost:37017 - HostUnreachable: End of file
2018-04-30T18:38:15.828+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Dropping all pooled connections to localhost:37017 due to failed operation on a connection
2018-04-30T18:38:15.828+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-3-0] Failed to close stream: Transport endpoint is not connected
2018-04-30T18:38:18.638+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Failed asio heartbeat to localhost:37017 - HostUnreachable: End of file
2018-04-30T18:38:18.638+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Dropping all pooled connections to localhost:37017 due to failed operation on a connection
2018-04-30T18:38:18.638+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-1-0] Failed to close stream: Transport endpoint is not connected
2018-04-30T18:38:20.004+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Failed asio heartbeat to localhost:37017 - HostUnreachable: End of file
2018-04-30T18:38:20.006+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Dropping all pooled connections to localhost:37017 due to failed operation on a connection
2018-04-30T18:38:20.006+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Failed to close stream: Transport endpoint is not connected
2018-04-30T18:38:25.972+0000 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 60 secs, remote host 127.0.0.1:37017)
2018-04-30T18:38:25.978+0000 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 60 secs, remote host 127.0.0.1:37017)
2018-04-30T18:38:29.583+0000 I SHARDING [conn4] Refreshing chunks for collection accidents.accidents based on version 5|10||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:29.585+0000 I SHARDING [ConfigServerCatalogCacheLoader-0] Refresh for collection accidents.accidents took 3 ms and found version 6|1||5ae7620e1b1b0bfabb4ae6f5
2018-04-30T18:38:29.597+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:29.606+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:29.606+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 3 checks in a row.
2018-04-30T18:38:30.107+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:30.107+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:30.107+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 4 checks in a row.
2018-04-30T18:38:30.608+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:30.608+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:30.610+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 5 checks in a row.
2018-04-30T18:38:31.110+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:31.111+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:31.111+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 6 checks in a row.
2018-04-30T18:38:31.600+0000 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:31.600+0000 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Unable to reach primary for set s0
2018-04-30T18:38:31.601+0000 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 7 checks in a row.
2018-04-30T18:38:31.612+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:31.613+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:31.613+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 8 checks in a row.
2018-04-30T18:38:32.114+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:32.114+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:32.114+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 9 checks in a row.
2018-04-30T18:38:32.615+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:32.616+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:32.616+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 10 checks in a row.
2018-04-30T18:38:33.118+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:33.120+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:33.120+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 11 checks in a row.
2018-04-30T18:38:33.621+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:33.621+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:34.123+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:34.124+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:34.624+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:34.626+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:35.127+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:35.128+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:35.628+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:35.629+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:36.129+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:36.130+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:36.630+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:36.631+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:37.132+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:37.132+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:37.633+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:37.633+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:38.134+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:38.135+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:38.135+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 21 checks in a row.
2018-04-30T18:38:38.636+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:38.637+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:39.138+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:39.138+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:39.640+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:39.640+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:40.142+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:40.142+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:40.644+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:40.645+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:41.146+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:41.146+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:41.647+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:41.648+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:42.148+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:42.148+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:42.651+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:42.651+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:43.152+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:43.152+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:43.152+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 31 checks in a row.
2018-04-30T18:38:43.653+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:43.653+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:44.154+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:44.156+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:44.656+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:44.657+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:45.158+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:45.158+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:45.659+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:45.660+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:46.161+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:46.163+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:46.664+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:46.664+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:47.165+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:47.165+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:47.666+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:47.666+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:48.166+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:48.168+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:48.168+0000 I NETWORK  [conn4] Cannot reach any nodes for set s0. Please check network connectivity and the status of the set. This has happened for 41 checks in a row.
2018-04-30T18:38:48.669+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:48.670+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:49.171+0000 W NETWORK  [conn4] Failed to connect to 127.0.0.1:37017, in(checking socket for error after poll), reason: Connection refused
2018-04-30T18:38:49.171+0000 W NETWORK  [conn4] Unable to reach primary for set s0
2018-04-30T18:38:49.173+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Connecting to localhost:47017
2018-04-30T18:38:49.190+0000 I ASIO     [NetworkInterfaceASIO-TaskExecutorPool-0-0] Successfully connected to localhost:47017, took 18ms (1 connections now open to localhost:47017)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn7] end connection 127.0.0.1:33902 (6 connections now open)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn5] end connection 127.0.0.1:33894 (5 connections now open)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn8] end connection 127.0.0.1:33906 (7 connections now open)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn9] end connection 127.0.0.1:33930 (4 connections now open)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn6] end connection 127.0.0.1:33898 (3 connections now open)
2018-04-30T18:38:57.009+0000 I NETWORK  [conn4] end connection 127.0.0.1:33882 (2 connections now open)
2018-04-30T18:38:57.073+0000 I CONTROL  [signalProcessingThread] got signal 15 (Terminated), will terminate after current cmd ends
2018-04-30T18:38:57.105+0000 W SHARDING [signalProcessingThread] error encountered while cleaning up distributed ping entry for tm351-18J-test-student:27017:1525113323:-3961489942162703859 :: caused by :: InterruptedAtShutdown: interrupted at shutdown
2018-04-30T18:38:57.106+0000 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-04-30T18:38:57.114+0000 I CONTROL  [signalProcessingThread] shutting down with code:0
